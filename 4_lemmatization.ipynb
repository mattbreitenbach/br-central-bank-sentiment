{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01038673",
   "metadata": {},
   "source": [
    "#### Lemmatization of COPOM Minutes\n",
    "\n",
    "This notebook prepares textual data for Natural Language Processing (NLP) by performing lemmatization.\n",
    " * The script tokenizes the raw text, removes common English stop words and non-alphabetic characters, and reduces words to their base or dictionary form (lemmatization).\n",
    " * It reads text files from the data/processed/copom_minutes_txt folder and saves the processed, lemmatized text into the data/processed/copom_minutes_lemmatized folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb73b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_MINUTES_TXT = \"./data/processed/copom_minutes_txt\"\n",
    "FOLDER_MINUTES_LEMMATIZED = \"./data/processed/copom_minutes_lemmatized\"\n",
    "FOLDER_MINUTES_NOT_LEMMATIZED = \"./data/processed/copom_minutes_not_lemmatized\"\n",
    "\n",
    "INITIAL_DATE = \"2003-06-26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29244f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes_info = pd.read_excel(\"./data/raw/minutes_info.xlsx\")\n",
    "minutes_info['DataReferencia'] = pd.to_datetime(minutes_info['DataReferencia'])\n",
    "minutes_info = minutes_info[minutes_info[\"DataReferencia\"] >= INITIAL_DATE]\n",
    "\n",
    "minutes_names = minutes_info[\"Titulo\"].to_list()\n",
    "filepaths = [f\"{FOLDER_MINUTES_TXT}/{minute}.txt\" for minute in minutes_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905206f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    This helper function is necessary for accurate lemmatization.\n",
    "    Converting a Penn Treebank POS tag to a WordNet POS tag.\n",
    "\n",
    "    Parameters:\n",
    "    - treebank_tag (str): The POS tag from nltk.pos_tag().\n",
    "\n",
    "    Returns:\n",
    "    - str: The corresponding WordNet POS tag. Returns 'n' (noun) by default if the tag is not a recognized.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed1f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes, removes stop words, and lemmatizes a given string of text.\n",
    "\n",
    "    The function first converts the text to lowercase, then tokenizes it. It\n",
    "    filters out stop words and non-alphabetic tokens before performing tagging and lemmatization.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The raw text to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of lemmatized words, with stop words and punctuation removed.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for word, tag in pos_tagged_tokens:\n",
    "        if word not in stop_words_english and word.isalpha() and len(word) > 1:\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "            \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed3e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in filepaths:\n",
    "    filename = os.path.basename(path)\n",
    "        \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "    \n",
    "    with open(f\"{FOLDER_MINUTES_NOT_LEMMATIZED}/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        for paragraph in paragraphs:\n",
    "            f.write(paragraph + '\\n')\n",
    "\n",
    "    lemm_paragraphs = [preprocess_text(p) for p in paragraphs]\n",
    "    with open(f\"{FOLDER_MINUTES_LEMMATIZED}/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        for paragraph in lemm_paragraphs:\n",
    "            line = ' '.join(paragraph)\n",
    "            f.write(line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
